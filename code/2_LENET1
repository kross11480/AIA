{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LeNet-1 (1989) Implementation in JAX\n\nThis notebook is reimplementation of the original Convnet paper from LeCun et.al used for digit recogntion in handwritten zip codes. \n\n## Table of Contents\n1. [Data Preparation](#data-preparation)\n2. [Model Architecture](#model-architecture)\n3. [Forward Pass](#forward-pass)\n4. [Training Loop](#training-loop)\n5. [Results](#results)\n\n\nSundry Concepts: \n* Shape transformations: print(x.shape) important debugging tool. assertion on parameter counts, macs help catch errors\n  ```\n    Input:        (batch, 16, 16, 1)\n    ↓ Pad + Conv\n    After H1:     (batch, 12, 8, 8)\n    ↓ Pad + Conv\n    After H2:     (batch, 12, 4, 4)\n    ↓ Flatten\n    Before H3:    (batch, 192)\n    ↓ FC\n    After H3:     (batch, 30)\n    ↓ FC\n    Output:       (batch, 10)\n    ```\n* Dimension notations: NHWC (batch, height, width, channels) (Tensor Flow style, NCHW is Pytorch/JAX, OIHW for kernels)","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation\n* Load, reshape, and resize 16x16 image from 28x28 image in MNIST Dataset\n* Dataset contains 60000 training and 10000 test images of handwritten digits and corresponding labels (0-9)\n* Batch Processing: TensorFlow's tf.image.resize() expects 4D tensors, hence new axis to add channel dimension: Required shape: (batch, height, width, channels)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# plt.imshow(x_train[0], cmap = 'gray')\n#print('label', y_train[0])\n#print (x_train.shape)\n\n# Add channels\" dimensions\nx_train = x_train[..., tf.newaxis]\nx_test = x_test[..., tf.newaxis]\n\nx_train  = tf.image.resize(x_train, [16,16])\nx_test  = tf.image.resize(x_test, [16,16])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:37:56.186095Z","iopub.execute_input":"2025-12-13T10:37:56.186665Z","iopub.status.idle":"2025-12-13T10:37:56.819699Z","shell.execute_reply.started":"2025-12-13T10:37:56.186629Z","shell.execute_reply":"2025-12-13T10:37:56.818596Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"* save first 7291 training image data(16x16) and 2007 test image data after scaling pixels to [-1, 1]\n* the labels are one-hot coded. Background is -1. e.g. 7 is [-1,-1,-1,-1,-1,-1,-1, 1,-1,-1]\n* saved to npz file","metadata":{}},{"cell_type":"code","source":"dataset = {'train': (x_train, y_train, 7291),\n           'test': (x_test, y_test, 2007)}\nfor split_name, (x_raw, y_raw, n) in dataset.items():\n    # create empty array\n    X = np.zeros((n, 16, 16, 1), dtype=np.float32)\n    Y = -np.ones((n, 10), dtype=np.float32)\n    imgs = x_raw[0:n]\n    imgs = imgs / 127.5 - 1.0\n    X[:] = imgs\n    ys = y_raw[0:n]\n    for i in range(n):\n        Y[i, ys[i]] = 1.0\n    # save in a portable format\n    np.savez(f\"{split_name}1989.npz\", X=X, Y=Y)\n\n    print(f\"Saved {split_name}1989.npz    shape X={X.shape}, Y={Y.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:37:56.821318Z","iopub.execute_input":"2025-12-13T10:37:56.821634Z","iopub.status.idle":"2025-12-13T10:37:56.873944Z","shell.execute_reply.started":"2025-12-13T10:37:56.821603Z","shell.execute_reply":"2025-12-13T10:37:56.872988Z"}},"outputs":[{"name":"stdout","text":"Saved train1989.npz    shape X=(7291, 16, 16, 1), Y=(7291, 10)\nSaved test1989.npz    shape X=(2007, 16, 16, 1), Y=(2007, 10)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"* load data from npz file\n* Convert numpy array to jax arrays.\n* JAX (similar to pytorch) provides TPU/GPU acceleration","metadata":{}},{"cell_type":"code","source":"import jax.numpy as jnp\ndata_train = np.load(\"train1989.npz\")\ndata_test = np.load(\"test1989.npz\")\n\n# Access arrays inside\nXtr = data_train[\"X\"]\nYtr = data_train[\"Y\"]\n\nXte = data_test[\"X\"]\nYte = data_test[\"Y\"]\n\nX_tr = jnp.array(Xtr)\nY_tr = jnp.array(Ytr)\nX_te = jnp.array(Xte)\nY_te = jnp.array(Yte)\n\nprint(f\"Training data shape: {X_tr.shape}\")\nprint(f\"Training labels shape: {Y_tr.shape}\")\nprint(f\"Test data shape: {X_te.shape}\")\nprint(f\"Test labels shape: {Y_te.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:37:56.874898Z","iopub.execute_input":"2025-12-13T10:37:56.875225Z","iopub.status.idle":"2025-12-13T10:37:56.905098Z","shell.execute_reply.started":"2025-12-13T10:37:56.875199Z","shell.execute_reply":"2025-12-13T10:37:56.904168Z"}},"outputs":[{"name":"stdout","text":"Training data shape: (7291, 16, 16, 1)\nTraining labels shape: (7291, 10)\nTest data shape: (2007, 16, 16, 1)\nTest labels shape: (2007, 10)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"# %%\nimport os\nimport json\nimport numpy as np\nimport jax\nfrom jax import grad, jit, random\nimport optax\nfrom flax import linen as nn\nimport torch  # only needed to load the .pt files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:37:56.906896Z","iopub.execute_input":"2025-12-13T10:37:56.907202Z","iopub.status.idle":"2025-12-13T10:37:56.912388Z","shell.execute_reply.started":"2025-12-13T10:37:56.907173Z","shell.execute_reply":"2025-12-13T10:37:56.911349Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"* Weight initialization requires fan_in(number of input connections to a neuron) which prevents vanishing gradients.\n* LeNet-1 Network Class\n    * use jax.lax.conv_general_dilated with:\n    * image batch: (7291, 20, 20, 1)→ interpreted as NHWC (batch, height, width, channels)\n    * weights: (12, 1, 5, 5)→ interpreted as (out_channels, in_channels, filter_height, filter_width)\n    * you must tell JAX which dimension format you’re using. Correct usage: NHWC input, OIHW kernel. Output dimension same as bias, NCHW\n* Architecture\n    * #Parameters: 9790\n    * #macs: 64,660\n    * Layer: Conv-Conv-FC-FC","metadata":{}},{"cell_type":"code","source":"def init_weights(key, fan_in, *shape):\n    \"\"\"Weight initialization as described in the paper\"\"\"\n    return (random.uniform(key, shape) - 0.5) * 2 * 2.4 / fan_in\n\nclass Net:\n    \"\"\"1989 LeCun ConvNet per description in the paper\"\"\"\n    \n    def __init__(self, key):\n        keys = random.split(key, 4)\n        \n        macs = 0  # keep track of MACs (multiply accumulates)\n        acts = 0  # keep track of number of activations\n        \n        # H1 layer parameters\n        self.H1w = init_weights(keys[0], 5*5*1, 12, 1, 5, 5)\n        self.H1b = jnp.zeros((12, 8, 8))\n        assert self.H1w.size + self.H1b.size == 1068\n        macs = 5*5*8*8*12 + 768\n        acts += (8*8) * 12\n        assert macs == 19968\n\n\n        # H2 layer parameters\n        self.H2w = init_weights(keys[1], 5*5*8, 12, 8, 5, 5)\n        self.H2b = jnp.zeros((12, 4, 4))\n        assert self.H2w.size + self.H2b.size == 2592\n        macs += (5*5*8) * (4*4) * 12 + 192\n        acts += (4*4) * 12\n        assert macs == 58560\n\n        # H3 fully connected layer\n        self.H3w = init_weights(keys[2], 4*4*12, 4*4*12, 30)\n        self.H3b = jnp.zeros(30)\n        assert self.H3w.size + self.H3b.size == 5790\n        macs += (4*4*12) * 30 + 30\n        acts += 30\n        assert macs == 64350\n\n        # Output layer\n        self.outw = init_weights(keys[3], 30, 30, 10)\n        self.outb = -jnp.ones(10)\n        assert self.outw.size + self.outb.size == 310\n        macs += 30 * 10 + 10\n        acts += 10\n        assert macs == 64660\n        self.macs = macs\n        self.acts = acts\n        \n        # Pack all parameters into a dict\n        self.params = {\n            'H1w': self.H1w,\n            'H1b': self.H1b,\n            'H2w': self.H2w,\n            'H2b': self.H2b,\n            'H3w': self.H3w,\n            'H3b': self.H3b,\n            'outw': self.outw,\n            'outb': self.outb,\n        }\n    \n    def count_params(self):\n        return sum(p.size for p in self.params.values())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:37:56.913282Z","iopub.execute_input":"2025-12-13T10:37:56.913793Z","iopub.status.idle":"2025-12-13T10:37:56.930624Z","shell.execute_reply.started":"2025-12-13T10:37:56.913767Z","shell.execute_reply":"2025-12-13T10:37:56.929290Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Forward pass\n* Padding: add border pixels to Height and Width Dimension (NHWC)\n* Stride = 2 in height and width dimension, which reduces output size by 4\n* Partial Connectivity: Using the same scheme as in Karpathy implementation\n* Tanh activation in all layer","metadata":{}},{"cell_type":"code","source":"def forward(params, x):\n    \"\"\"Forward pass of the network\"\"\"\n    # x has shape (batch, 16, 16, 1)\n    x = jnp.pad(x, ((0, 0), (2, 2), (2, 2), (0, 0)), constant_values=1.0)\n    \n    x = jax.lax.conv_general_dilated(x, params['H1w'], window_strides=(2, 2), \n                                     padding='VALID', \n                                     dimension_numbers=('NHWC', 'OIHW', 'NCHW')) + params['H1b']\n    x = jnp.tanh(x)\n\n    # x is now shape (batch, 12, 8, 8)\n    x = jnp.pad(x, ((0, 0), (0, 0), (2, 2), (2, 2)), constant_values=-1.0)\n    slice1 = jax.lax.conv_general_dilated(\n        x[:, 0:8], params['H2w'][0:4], window_strides=(2, 2),\n        padding='VALID', dimension_numbers=('NCHW', 'OIHW', 'NCHW')\n    )\n    slice2 = jax.lax.conv_general_dilated(\n        x[:, 4:12], params['H2w'][4:8], window_strides=(2, 2),\n        padding='VALID', dimension_numbers=('NCHW', 'OIHW', 'NCHW')\n    )\n    slice3 = jax.lax.conv_general_dilated(\n        jnp.concatenate([x[:, 0:4], x[:, 8:12]], axis=1), params['H2w'][8:12],\n        window_strides=(2, 2), padding='VALID', dimension_numbers=('NCHW', 'OIHW', 'NCHW')\n    )\n    x = jnp.concatenate([slice1, slice2, slice3], axis=1) + params['H2b']\n    x = jnp.tanh(x)\n\n    # x is now shape (batch, 12, 4, 4)\n    x = x.reshape(x.shape[0], -1)  # (batch, 12*4*4)\n    x = x @ params['H3w'] + params['H3b']\n    x = jnp.tanh(x)\n    \n    # x is now shape (batch, 30)\n    x = x @ params['outw'] + params['outb']\n    x = jnp.tanh(x)\n    \n    # x is finally shape (batch, 10)\n    \n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:37:56.931528Z","iopub.execute_input":"2025-12-13T10:37:56.931773Z","iopub.status.idle":"2025-12-13T10:37:56.953980Z","shell.execute_reply.started":"2025-12-13T10:37:56.931755Z","shell.execute_reply":"2025-12-13T10:37:56.952347Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Training Loop\n* Hyperparameters: num epochs, seed\n* Loss Function: Mean Squared Error (loss estimation between prediction and target)\n* Training step:\n    - Parameters are weights, bias at each layer\n    - `jax.value_and_grad`: Computes both loss value and gradients. JAX computes gradient by tracing the computation of loss function and therefore does not need model graph (Network is a function).\n    - `optimizer.update`: Calculates how to change parameters\n    - `optax.apply_updates`: Actually updates the parameters\n  * Initialize training","metadata":{}},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport optax\nfrom functools import partial\n\n# Parameters\nseed = 1337\nnum_epochs = 23\n\n@jax.jit\ndef loss_fn(params, x, y):\n    yhat = forward(params, x)\n    return jnp.mean((y - yhat) ** 2)\n\n# ---------------------------------------------------------\n# Train-step with optimizer STATIC\n# ---------------------------------------------------------\n@partial(jax.jit, static_argnums=0)\ndef train_step(optimizer, params, opt_state, x, y):\n    # Compute loss + gradient\n    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n\n    # Compute optimizer updates\n    updates, opt_state = optimizer.update(grads, opt_state, params)\n    params = optax.apply_updates(params, updates)\n\n    return params, opt_state, loss\n\n\n\n# Initialize random seed\nkey = random.PRNGKey(seed)\nnp.random.seed(seed)\n\n# Create model\nmodel = Net(key)\nparams = model.params\n\n# init optimizer\noptimizer = optax.sgd(.03)\nopt_state = optimizer.init(params)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:37:56.955015Z","iopub.execute_input":"2025-12-13T10:37:56.955364Z","iopub.status.idle":"2025-12-13T10:37:56.991093Z","shell.execute_reply.started":"2025-12-13T10:37:56.955331Z","shell.execute_reply":"2025-12-13T10:37:56.989729Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"* Evaluation function\n* Main training loop","metadata":{}},{"cell_type":"code","source":"def eval_split(split, pass_num):\n    \"\"\"Evaluate the full train/test set\"\"\"\n    X, Y = (X_tr, Y_tr) if split == 'train' else (X_te, Y_te)\n    Yhat = forward(params, X)\n    loss = jnp.mean((Y - Yhat)**2)\n    err = jnp.mean((Y.argmax(axis=1) != Yhat.argmax(axis=1)).astype(float))\n    misses = int(err.item() * Y.shape[0])\n    \n    print(f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {misses}\")\n    \n    return loss.item(), err.item(), misses\n    \n# Train for specified number of epochs\nfor pass_num in range(23):\n    \n    # Perform one epoch of training\n    for step_num in range(X_tr.shape[0]):\n        \n        # Fetch a single example into a batch of 1\n        x, y = X_tr[step_num:step_num+1], Y_tr[step_num:step_num+1]\n        \n        # Training step\n        params, opt_state, loss = train_step(optimizer, params, opt_state, x, y)\n    \n    # After each epoch evaluate the train and test error/metrics\n    print(f\"\\nEpoch {pass_num + 1}/{num_epochs}\")\n    train_loss, train_err, train_misses = eval_split('train', pass_num)\n    test_loss, test_err, test_misses = eval_split('test', pass_num)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:37:56.992090Z","iopub.execute_input":"2025-12-13T10:37:56.992373Z","iopub.status.idle":"2025-12-13T10:39:35.844243Z","shell.execute_reply.started":"2025-12-13T10:37:56.992347Z","shell.execute_reply":"2025-12-13T10:39:35.842990Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/23\neval: split train. loss 2.443244e-01. error 38.79%. misses: 2828\neval: split test . loss 2.621335e-01. error 44.00%. misses: 883\n\nEpoch 2/23\neval: split train. loss 6.739677e-02. error 9.94%. misses: 724\neval: split test . loss 8.985768e-02. error 14.00%. misses: 281\n\nEpoch 3/23\neval: split train. loss 4.163871e-02. error 6.19%. misses: 450\neval: split test . loss 6.399573e-02. error 9.12%. misses: 183\n\nEpoch 4/23\neval: split train. loss 3.623268e-02. error 5.36%. misses: 390\neval: split test . loss 5.836362e-02. error 8.92%. misses: 179\n\nEpoch 5/23\neval: split train. loss 3.041344e-02. error 4.20%. misses: 306\neval: split test . loss 5.369483e-02. error 7.82%. misses: 157\n\nEpoch 6/23\neval: split train. loss 3.036772e-02. error 4.47%. misses: 325\neval: split test . loss 5.582738e-02. error 8.17%. misses: 164\n\nEpoch 7/23\neval: split train. loss 2.258119e-02. error 3.26%. misses: 237\neval: split test . loss 4.819264e-02. error 7.03%. misses: 140\n\nEpoch 8/23\neval: split train. loss 2.116228e-02. error 2.89%. misses: 211\neval: split test . loss 4.682430e-02. error 6.78%. misses: 135\n\nEpoch 9/23\neval: split train. loss 2.016818e-02. error 2.81%. misses: 204\neval: split test . loss 4.722514e-02. error 6.93%. misses: 139\n\nEpoch 10/23\neval: split train. loss 1.627025e-02. error 2.22%. misses: 161\neval: split test . loss 4.462180e-02. error 6.63%. misses: 133\n\nEpoch 11/23\neval: split train. loss 1.806669e-02. error 2.51%. misses: 183\neval: split test . loss 4.703502e-02. error 6.88%. misses: 138\n\nEpoch 12/23\neval: split train. loss 1.748127e-02. error 2.50%. misses: 181\neval: split test . loss 4.745530e-02. error 7.08%. misses: 142\n\nEpoch 13/23\neval: split train. loss 1.686935e-02. error 2.40%. misses: 174\neval: split test . loss 4.811700e-02. error 7.08%. misses: 142\n\nEpoch 14/23\neval: split train. loss 1.285756e-02. error 1.66%. misses: 121\neval: split test . loss 4.429949e-02. error 6.78%. misses: 135\n\nEpoch 15/23\neval: split train. loss 8.910766e-03. error 1.08%. misses: 78\neval: split test . loss 4.064344e-02. error 6.28%. misses: 125\n\nEpoch 16/23\neval: split train. loss 9.057206e-03. error 1.17%. misses: 84\neval: split test . loss 4.175927e-02. error 6.33%. misses: 127\n\nEpoch 17/23\neval: split train. loss 1.016162e-02. error 1.40%. misses: 101\neval: split test . loss 4.421616e-02. error 6.98%. misses: 140\n\nEpoch 18/23\neval: split train. loss 6.914968e-03. error 0.86%. misses: 63\neval: split test . loss 4.059896e-02. error 6.18%. misses: 124\n\nEpoch 19/23\neval: split train. loss 5.875030e-03. error 0.70%. misses: 50\neval: split test . loss 3.975138e-02. error 5.83%. misses: 117\n\nEpoch 20/23\neval: split train. loss 5.068413e-03. error 0.62%. misses: 45\neval: split test . loss 3.750712e-02. error 5.63%. misses: 113\n\nEpoch 21/23\neval: split train. loss 4.683753e-03. error 0.56%. misses: 41\neval: split test . loss 3.678763e-02. error 5.53%. misses: 111\n\nEpoch 22/23\neval: split train. loss 4.316389e-03. error 0.55%. misses: 40\neval: split test . loss 3.635434e-02. error 5.48%. misses: 110\n\nEpoch 23/23\neval: split train. loss 3.984636e-03. error 0.56%. misses: 41\neval: split test . loss 3.577922e-02. error 5.38%. misses: 108\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## Results\n\nAfter epoch 23, the first samples taken and not randomized\n* fan_in ** 0.5\n    * eval: split train. loss 4.716225e-03. error 0.78%. misses: 57\n    * eval: split test . loss 4.105802e-02. error 6.18%. misses: 124\n* fan_in\n    * eval: split train. loss 3.984636e-03. error 0.56%. misses: 41\n    * eval: split test . loss 3.577922e-02. error 5.38%. misses: 108\n* After 7291 to 145.. sample\n    * eval: split train. loss 7.991600e-03. error 1.08%. misses: 78\n    * eval: split test . loss 3.900198e-02. error 5.63%. misses: 113","metadata":{}}]}